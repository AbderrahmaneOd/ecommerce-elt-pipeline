# E-Commerce ELT Pipeline

## ğŸ“„ Description
This project implements an ELT (Extract, Load, Transform) data pipeline for processing and analyzing e-commerce transaction data from a UK-based online retailer. Using a modern tech stack, the pipeline enables efficient data ingestion, transformation, and visualization to generate actionable business insights. The entire stack is containerized using Docker for ease of deployment.

## ğŸš€ Features
- **Data Extraction**: Ingest raw e-commerce transaction data from CSV files.
- **Data Transformation**: Clean, aggregate, and enrich data using Apache Spark.
- **Data Warehousing**: Store processed data in Hive tables for querying.
- **Workflow Orchestration**: Manage pipeline tasks using Apache Airflow.
- **Scalability**: Leverage Hadoop for distributed data storage and processing.
- **Containerized Deployment**: Use Docker to run the entire stack seamlessly.

## ğŸ› ï¸ Tech Stack
- **Apache Spark**: For distributed data processing and transformation.
- **Apache Hive**: As the data warehousing solution.
- **Apache Airflow**: For workflow orchestration and scheduling.
- **Hadoop**: For distributed storage (HDFS).
- **Docker**: For containerization of the pipeline.

## ğŸ“‚ Project Structure




## âš™ï¸ Setup and Usage

### Prerequisites
- Docker and Docker Compose installed
- Basic understanding of ELT pipelines and the tech stack

### Steps
1. **Clone the Repository**
   ```bash
   git clone https://github.com/AbderrahmaneOd/spark-hive-airflow-ecommerce-pipeline.git
   cd spark-hive-airflow-ecommerce-pipeline
   ```

2. **Clone the Repository**
   ```bash
   docker compose up -d
   ```


3. **Access Services**

4. **Run the Pipeline**

5. **Run the Pipeline**

## ğŸ§ª Example Queries


