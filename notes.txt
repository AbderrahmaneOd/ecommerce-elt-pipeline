Namenode: http://localhost:9870/dfshealth.html#tab-overview
Namenode(IPC port): http://localhost:9000
History server: http://localhost:8188/applicationhistory
Datanode: http://localhost:9864/
Nodemanager: http://localhost:8042/node
Resource manager: http://localhost:8088/
Hue: http://localhost:8888
Spark Master UI: http://localhost:8080
Spark Slave UI: http://localhost:8081
Spark Driver UI: http://localhost:4040(accessible only after a driver is started)
Zeppelin UI: http://localhost:8082
Airflow UI: http://localhost:3000

Resources on setting up services in docker:
- HDFS & YARN ==> https://github.com/big-data-europe/docker-hadoop/blob/master/docker-compose.yml
- Hive ==> https://github.com/big-data-europe/docker-hive/blob/master/docker-compose.yml
- Hue ==> https://github.com/cloudera/hue/blob/master/tools/docker/hue/docker-compose.yml

HDFS commands
- hdfs dfs -ls / ==> shows list of file/folder in root
- hdfs dfs -mkdir /user ==> create a folder called user
- hdfs dfs -touchz /user/test.txt ==> create a test.txt file inside user folder
- hdfs dfs -cat /user/test.txt ==> read data inside /user/test.txt
- hdfs dfs -stat /user/test.txt ==> show stats of test.txt file
- hdfs dfs -appendToFile test.txt /user/test.txt ==> copy content from local file to hdfs file
- hdfs dfs -copyFromLocal <local file path>  <dest(present on hdfs)> ==> copy file from local file to hdfs file
- hdfs dfs -copyToLocal <local file path>  <dest(present on hdfs)> ==> copy hdfs file to local file
- hdfs dfs -du /user/test.txt ==> shows file size
result => 18  54  /user/test.txt
18 is 1 block size and since replication factor is 3 so 18*3 = 54. Therefore 54 is total block size in hdfs
- hdfs dfs -setrep -R -w 6 /user/test.txt ==> set replication factor of test.txt file to 6. Used for file
- hdfs dfs -setrep -R 4 /user ==> set replication factor of /user file to 6. User for folder

### HIVE ###
# create managed ratings table #
CREATE TABLE IF NOT EXISTS ratings (
    userId INT,
    movieId INT,
    rating INT,
    time INT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

LOAD DATA INPATH '/user/admin/learning/hive/u.data'
OVERWRITE INTO TABLE ratings;
-> this will import u.data & will create a folder ratings in /user/hive/warehouse directory in HDFS and all the data will be stored inside this directory i.e.  /user/hive/warehouse/ratings

## create external ratings table ##
CREATE EXTERNAL TABLE IF NOT EXISTS ratings (
    userId INT,
    movieId INT,
    rating INT,
    time INT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE
LOCATION '/user/admin/learning/hive/externally_managed_ratings_table';

-> This will create another directory on hdfs.
-> If you copy u.data file on hdfs to this new directory then you can see the results when you query in hive
-> run below command after entering into a node to copy u.data to externally_managed_ratings_table directory
-> hdfs dfs -cp  /user/admin/learning/hive/u.data /user/admin/learning/hive/externally_managed_ratings_table
-> managed table would import data stored on hdfs to hive warehouse directory(/user/hive/warehouse) on hdfs. If table is dropped then the associated metadata & schema is deleted from metastore & the data on hdfs also gets deleted.
-> when a external table is dropped it deletes the metadata & schema from metastore but the actual files are not deleted from hdfs

## create an external movies table ##
CREATE EXTERNAL TABLE IF NOT EXISTS movies (
    movieId INT,
    name VARCHAR(100),
    publishDate VARCHAR(50)
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
STORED AS TEXTFILE
LOCATION '/user/admin/learning/hive/externally_managed_movies_table';

-> hdfs dfs -cp  /user/admin/learning/hive/u.item /user/admin/learning/hive/externally_managed_movies_table

# query to get popular movies #

SELECT movieId, name, rating_counts_table.ratingCount
FROM movies
JOIN (
    SELECT movieId as id, count(movieId) AS ratingCount
    FROM ratings
    GROUP BY movieId
    ORDER BY ratingCount DESC
) AS rating_counts_table
ON movies.movieid = rating_counts_table.id;

# save query results in hdfs #
INSERT OVERWRITE DIRECTORY '/user/admin/learning/hive/outputs'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
SELECT movieId, name, rating_counts_table.ratingCount
FROM movies
JOIN (
    SELECT movieId as id, count(movieId) AS ratingCount
    FROM ratings
    GROUP BY movieId
    ORDER BY ratingCount DESC
) AS rating_counts_table
ON movies.movieid = rating_counts_table.id;

### SQOOP ###

-> log into hive server
-> https://archive.apache.org/dist/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
-> cd /
-> curl -O https://archive.apache.org/dist/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
-> tar -xf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
-> rm -rf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
-> mv sqoop-1.4.7.bin__hadoop-2.6.0/* /usr/lib/sqoop 
// ignore: -> echo "export SQOOP_HOME=/usr/lib/sqoop" >> ~/.bashrc
// ignore: -> echo "export PATH=$PATH:$SQOOP_HOME/bin" >> ~/.bashrc
-> source ~/.bashrc
//ignore: -> cd $SQOOP_HOME/conf  
-> apt-get update
-> apt-get install nano -y --force-yes

-> cd /usr/lib/sqoop/lib
-> curl -O http://ftp.ntu.edu.tw/MySQL/Downloads/Connector-J/mysql-connector-java-5.1.49.tar.gz
-> tar -xf mysql-connector-java-5.1.49.tar.gz
-> rm -rf mysql-connector-java-5.1.49.tar.gz
-> curl -O https://jdbc.postgresql.org/download/postgresql-42.5.0.jar --insecure
-> curl -O jdbc.postgresql.org/download/postgresql-9.3-1101.jdbc41.jar
-> curl -O https://repo1.maven.org/maven2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar
-> curl -O https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/org-json-java/org.json-20120521.jar

-> curl -O https://repo1.maven.org/maven2/org/apache/hive/hive-common/2.3.2/hive-common-2.3.2.jar

-> cd /usr/lib/sqoop
// ignore this command -> cp /opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar /usr/lib/sqoop/
-> curl -O https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/3.2.1/hadoop-mapreduce-client-jobclient-3.2.1.jar

# install ssh server
-> apt-get install openssh-server -y --force-yes
-> service ssh start
# set os pass so that user can login via ssh
-> passwd root
-> set it to 1234

# Create a test table on external postgres DB #
CREATE TABLE IF NOT EXISTS Test(id SERIAL PRIMARY KEY, name VARCHAR(50));
INSERT INTO test (name) VALUES ('Mrugank Ray');

## Sqoop Import ##

-> https://data-flair.training/blogs/sqoop-import/
-> create a test table on postgres
-> sqoop import --connect jdbc:postgresql://external_postgres_db/external --username external --password external --table test --target-dir /user/admin/learning/sqoop/importing_test_from_postgre --m 1
-> directory specified under --target-dir should not exist. sqoop will create the directory and will store data there
-> if it says sqoop is unrecognized command then run cd /usr/lib/sqoop/bin and then add ./ before the sqoop command
-> sqoop uses YARN to distribute jobs. So, you need YARN to start a sqoop job.

-> create ratings table on postgres using pgadmin ui
-> click on tool and click on import/export on pgadmin
-> upload u.data and import u.data into ratings
-> if you want to join  multiple tables and import it using sqoop then add --query and specify the query to get required data in sqoop command but you need to specify the token $CONDITIONS in query so it can import results parallelly
-> example: sqoop import --query ‘SELECT a.*, b.* FROM a JOIN b on (a.id == b.id) WHERE $CONDITIONS’
-> you can add --where to add a condition in your import
-> sqoop import --connect jdbc:postgresql://external_postgres_db/external --username external -password external --table ratings --target-dir /user/admin/learning/sqoop/importing_ratings_from_postgre --where "rating = 1" -m 1

## Sqoop Inceremental Import ##

# all the other rows with rating that is not 1 is imported
-> good example of incremental imports: https://stackoverflow.com/questions/22969342/sqoop-incremental-import
-> sqoop import --connect jdbc:postgresql://external_postgres_db/external --username external --password external --table ratings --target-dir /user/admin/learning/sqoop/importing_ratings_from_postgre --check-column rating --incremental append --last-value 1 --m 1
-> --incremental append will import all the new rows
-> --incremental lastmodified will update the existing rows

## Sqoop Jobs ##
-> https://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html#_saved_jobs

# create a job
-> sqoop job --create <job-name> -- import ...other fields
-> example: sqoop job --create ratings_increment_import -- import --connect jdbc:postgresql://external_postgres_db/external --username external --password external --table ratings --target-dir /user/admin/learning/spark/importing_ratings_from_postgres --check-column id --incremental append --last-value 1 --m 1

# list sqoop jobs
-> sqoop job --list

# execute a job
-> sqoop job --exec <job-name>

# show job
-> sqoop job --show <job-name>

# delete the job
-> sqoop job --delete <job-name>

# list all jobs
-> sqoop job --list

-> when you execute a sqoop job, it will ask you for DB password unless you set sqoop.metastore.client.record.password to true in /usr/lib/sqoop/conf/sqoop-site.xml


## Sqoop Import Into Hive ##

-> sqoop import --connect jdbc:postgresql://external_postgres_db/external --username external -password external --table ratings --create-hive-table --hive-table ratings_managed --hive-import -m 1

## Sqoop Import Into Hive Into Partitions##
-> https://stackoverflow.com/questions/53485068/handle-partition-in-hive-table-while-using-sqoop-import#:~:text=you%20can%20import%20data%20directly,load%20it%20directly%20using%20sqoop.
-> https://community.cloudera.com/t5/Support-Questions/Creating-a-partitioned-table-in-hive-with-sqoop/td-p/295550
-> Sqoop can only insert into a single Hive partition at one time.
-> You need to add --hive-partition-key & --hive-partition-value in the import command
-> Only keep one value in --hive-partition-value & write a query & use WHERE condition to import rows where column is equal to hive-partition-value

## Sqoop Export ##

-> you need to create a table in your DB before you run the export on sqoop

-> Create a export_to_postgres table on external postgres DB first
CREATE TABLE IF NOT EXISTS export_to_postgres (id SERIAL PRIMARY KEY, name VARCHAR(50));

-> sqoop export --connect \
jdbc:postgresql://external_postgres_db/external \
 --table export_to_postgres \
 --username external --password external \
 --export-dir /user/admin/learning/sqoop \
 --m 1 \
 --input-fields-terminated-by ','

### SPARK ###

## setting up livy server(could not setup) ##
-> spark submit is here : /spark/bin
-> export SPARK_HOME=/spark
-> export JAVA_HOME=/usr
-> container didn't recognize spark-submit as command
-> curl -O https://dlcdn.apache.org/incubator/livy/0.7.1-incubating/apache-livy-0.7.1-incubating-bin.zip
-> unzip apache-livy-0.7.1-incubating-bin.zip -d /usr/bin/
-> mv apache-livy-0.7.1-incubating-bin livy
-> rm -rf apache-livy-0.7.1-incubating-bin.zip
-> pip3 install wget requests datawrangler
-> cd /usr/bin/livy/conf
-> apk add nano
-> nano livy.conf.template
livy.server.port = 8999
-> cd /usr/bin/livy/bin
-> ./livy-server start
-> 

https://hub.docker.com/r/uhopper/hadoop-spark
docker pull uhopper/hadoop-spark

-> master.sh is started
-> it starts runs these shell scripts
. "/spark/sbin/spark-config.sh"

. "/spark/bin/load-spark-env.sh"
-> load-spark-env.sh runs spark-env.sh in /conf directory if it exists
-> spark-config.sh starts py4j, which binds python to java
-> run spark in cluster mode with 
./spark/bin/spark-submit
    --jars=/home/hadoop/pg_jars/postgresql-9.4.1208.jre7.jar \
    --master yarn \
    --deploy-mode cluster \
    --driver-memory 4g \
    --executor-memory 2g \
    --executor-cores 1 \
    --queue <queue name> \
    --archives <external_compressed_libraries_file>
    <path to your python script>
-> https://spark.apache.org/docs/latest/running-on-yarn.html

-> set HADOOP_CONF_DIR & YARN_CONF_DIR

./spark-submit --master yarn --deploy-mode cluster --driver-memory 1g --executor-memory 1g --executor-cores 1 --queue default test.py

/etc/hadoop/yarn-site.xml

## DID NOT WORK ##
## Setup Spark in Hiver Server container ##
-> cd /opt
-> curl -O https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop2.tgz --insecure
-> tar xf spark-*
-> rm -rf spark-3.3.0-bin-hadoop2.tgz
-> mv spark-3.3.0-bin-hadoop2/* spark
-> rm -rf spark-3.3.0-bin-hadoop2/
-> apt-get update
-> apt-get install nano
-> apt-get -y install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libreadline-dev libffi-dev
-> cd /usr
-> curl -O https://www.python.org/ftp/python/3.6.9/Python-3.6.9.tgz
-> tar xf Python-3.6.9.tgz
-> cd Python-3.6.9
-> ./configure --enable-optimizations --enable-shared --with-ensurepip=install
-> make -j8
-> make altinstall
-> echo "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/python3" >> ~/.bashrc
-> apt-get -y install python3-pip
-> pip3 install typing
-> echo "export SPARK_HOME=/opt/spark" >> ~/.bashrc
-> echo "export PYSPARK_PYTHON=/usr/local/bin/python3.6" >> ~/.bashrc
-> echo "export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin" >> ~/.bashrc
-> cd /opt/spark/sbin
-> ./start-master.sh
-> cd /opt/spark/jars
-> curl -O https://repo1.maven.org/maven2/com/sun/jersey/jersey-client/1.19.4/jersey-client-1.19.4.jar
-> curl -O https://repo1.maven.org/maven2/com/sun/jersey/jersey-bundle/1.19.4/jersey-bundle-1.19.4.jar
-> cd /opt/spark/bin
-> ./spark-submit --master yarn --deploy-mode cluster --driver-memory 1g --executor-memory 1g --executor-cores 1 --queue default <python file path>


-> after running make -j8
make[1]: Entering directory '/usr/Python-3.6.9'
: # FIXME: can't run for a cross build
LD_LIBRARY_PATH=/usr/Python-3.6.9 ./python -m test.regrtest --pgo || true
Run tests sequentially

curl https://bootstrap.pypa.io/pip/3.6/get-pip.py | python3.6 - --user

apt-get install -y software-properties-common curl && add-apt-repository ppa:deadsnakes/ppa && apt-get update && apt-get install -y python3.6 python3.6-venv 

-> apt-get install -y make build-essential zlib1g-dev \
libbz2-dev libreadline-dev wget curl llvm libncurses5-dev \
libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python-openssl

-> unable to install python3.6 using pyenv on hive server

## DID NOT WORK ##
# Setup python 3.6 by pyenv #
-> log into namenode 
-> apt-get update
-> apt-get -y install git
-> apt-get -y install nano
-> apt-get install -y make build-essential libssl-dev zlib1g-dev \
libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev \
libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python-openssl
-> curl https://pyenv.run | bash
-> echo "export PYENV_ROOT=$HOME/.pyenv" >> ~/.bashrc
-> exit
-> login again
-> echo "export PATH=$PYENV_ROOT/bin:$PATH" >> ~/.bashrc
-> exit
-> login again
-> pyenv install -v 3.6.0
-> pyenv global 3.6.0
-> echo 'eval "$(pyenv init --path)"' >> ~/.bashrc
-> exit
-> login again

# setup spark3 on hadoop 3.2.1 with python 3.6 #
-> cd opt/
# spark archives: https://archive.apache.org/dist/spark/spark-3.2.0/
-> curl -O https://archive.apache.org/dist/spark/spark-3.2.2/spark-3.2.2-bin-hadoop3.2.tgz
-> /root/.pyenv/versions/3.6.0/bin/python3.6
-> tar xf spark-*
-> rm -rf spark*.tgz
-> mv spark-3.2.2-bin-hadoop3.2/ spark
-> echo "export SPARK_HOME=/opt/spark" >> ~/.bashrc
-> exit & login
-> echo "export PYSPARK_PYTHON=/usr/local/bin/python3.6" >> ~/.bashrc
-> exit & login
-> echo "export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin" >> ~/.bashrc
-> exit & login
-> cd /opt/spark/sbin
-> ./start-master.sh
-> cd /opt/spark/jars
-> curl -O https://repo1.maven.org/maven2/com/sun/jersey/jersey-client/1.19.4/jersey-client-1.19.4.jar
-> curl -O https://repo1.maven.org/maven2/com/sun/jersey/jersey-bundle/1.19.4/jersey-bundle-1.19.4.jar
-> cd /opt/spark/bin
-> ./spark-submit --master yarn --deploy-mode cluster --driver-memory 1g --executor-memory 1g --executor-cores 1 --queue default <python file path>

I don't have a lot of experience with pyspark or yarn, but if Polynote is supposed to use python3 but your executors need python, I think you want to set something like PYSPARK_DRIVER_PYTHON=python3 and PYSPARK_PYTHON=python (or the equivalent for yarn)

## WORKING ##
# Setup python 3.9 by conda #
-> login to namenode
-> cd opt/
-> curl -O https://repo.anaconda.com/miniconda/Miniconda3-py39_4.12.0-Linux-x86_64.sh
-> mv Mini* miniconda.sh
-> bash miniconda.sh -b -u -p ~/anaconda
// ignore: -> echo 'export PATH="~/anaconda/bin:$PATH"' >> ~/.bashrc 
-> source ~/.bashrc
-> conda update conda -y
-> rm miniconda.sh

# setup conda package. It helps packaging external libraries into a tar.gz file #
-> conda install conda-pack -y
# it will create pyspark_conda_env.tar.gz in the opt/ directory
-> conda pack -f -o pyspark_conda_env.tar.gz

# setup spark3 on hadoop 3.2.1 with python 3.9 #
-> cd opt/
# spark archives: https://archive.apache.org/dist/spark/spark-3.2.0/
-> curl -O https://archive.apache.org/dist/spark/spark-3.2.2/spark-3.2.2-bin-hadoop3.2.tgz
-> tar xf spark-*
-> rm -rf spark*.tgz
// ignore: -> mv spark-3.2.2-bin-hadoop3.2 spark
-> mv spark-3.2.2-bin-hadoop3.2/* spark
-> rm -rf spark-3.2.2-bin-hadoop3.2
// ignore: -> echo "export SPARK_HOME=/opt/spark" >> ~/.bashrc
// ignore: -> echo "export PYSPARK_PYTHON=/root/anaconda/bin/python3.9" >> ~/.bashrc
// ignore: -> source ~/.bashrc
// ignore: -> echo "export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin" >> ~/.bashrc
-> source ~/.bashrc
-> cd /opt/spark/jars
-> curl -O https://repo1.maven.org/maven2/com/sun/jersey/jersey-client/1.19.4/jersey-client-1.19.4.jar
-> curl -O https://repo1.maven.org/maven2/com/sun/jersey/jersey-bundle/1.19.4/jersey-bundle-1.19.4.jar
-> curl -O https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector-assembly_2.12/3.2.0/spark-cassandra-connector-assembly_2.12-3.2.0.jar
-> curl -O http://ftp.ntu.edu.tw/MySQL/Downloads/Connector-J/mysql-connector-java-5.1.49.tar.gz
-> tar -xf mysql-connector-java-5.1.49.tar.gz
-> rm -rf mysql-connector-java-5.1.49.tar.gz
-> curl -O https://jdbc.postgresql.org/download/postgresql-42.5.0.jar --insecure
// ignore: -> curl -O jdbc.postgresql.org/download/postgresql-9.3-1101.jdbc41.jar
-> cd /opt/spark/sbin
-> start-master.sh
-> start-slave.sh spark://namenode:7077
-> cd /opt/spark/bin
-> spark-submit --master yarn --deploy-mode cluster --driver-memory 1g --executor-memory 1g --executor-cores 1 --queue default --num-executors 1 --conf spark.sql.shuffle.partitions=200 --conf spark.default.parallelism=100 <python file path>

## YARN mode ##
# If you run spark job in yarn, then python needs to be installed in all of your machines(namenode, datanode, nodemanager & resourcemanager) in the same path.

## Stand Alone Mode ##
# You can run spark job in standalone mode also. You need to start master & slave node in spark. So, the job will run on slave node & master will watch the job status. Master has it's own resource manager so, it distributes the tasks.

## Local Mode ##
# You can run the job in local mode also. Just run this command
-> spark-submit <python file path>

# you can set --deploy-mode client if you want your driver script to run in the client machine. However if the client node is discinnected then the job will fail

# you can add --archives <file_name>.tar.gz to the spark submit command so that spark can ship your external dependencies to other nodes
-> spark-submit --master yarn --deploy-mode cluster --driver-memory 1g --executor-memory 1g --executor-cores 1 --queue default --archives <absolute_tar_file_path> <python file path>

# if you want to connect to external dbs using jdbc then you need to add --driver-class-path && --jars to the spark submit command(seprated by comma)
# postgres/mysql jdbc drivers are downloaded and stored in /opt/spark/jars directory.
# You need to specify the absolute path of jdbc driver for both the flags
-> spark-submit --driver-class-path /opt/spark/jars/postgresql-42.5.0.jar --jars /opt/spark/jars/postgresql-42.5.0.jar connect_to_postgres.py
# You can use --packages instead of --jars in case you dont't want to specify absolute paths to jar files. If you use --packages you will have to specify the package names & make sure all jars are inside /opt/spark/jars dir.
# difference between --packages & --jars -> https://stackoverflow.com/questions/51434808/spark-submit-packages-vs-jars#:~:text=%2D%2Djars%20JARS%20Comma%2Dseparated,repositories%20given%20by%20%2D%2Drepositories.

# You can add --conf spark.default.parallelism to create partitions in RDD. You can use spark.sql.shuffle.partitions to create partitions in DF.

# --executor-memory vs --executor-cores vs --num-executors
-> --num-executors = The value indicates the number of executors to launch.
-> --executor-cores = The value indicates the number of cores used by each executor.
-> --executor-memory = memory alocated to each executor

# read csv file in pyspark
-> https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/

# write csv file in spark
-> https://sparkbyexamples.com/spark/spark-write-dataframe-to-csv-file/

# You can save df by using .write and then specify the directory in .csv()
e.g. .write.csv("hdfs://namenode:9000/user/admin/learning/spark/average_rating_per_movie"). If you want to save the header then you need to specify this -> option("header", True)

# there is a mode function to specify save mode, default mode is "errorifexists" which throws error if the directory already exists. There are other save modes such as overwrite, append, ignore.

# read text file in pyspark
-> https://towardsdatascience.com/spark-essentials-how-to-read-and-write-data-with-pyspark-5c45e29227cd
-> https://www.geeksforgeeks.org/read-text-file-into-pyspark-dataframe/

# alias in spark sql works inside agg func or select func

# PySpark GroupBy Agg is a function in PySpark data model that is used to combine multiple Agg functions together and analyze the result.

## show
-> setting truncate=False in show will display the full content of the columns without truncation
-> when this func is called on df, it results in multiple jobs. It's better to avoid calling show in prod to avoid unnessary overheads.
-> In prod, when it's called, executors starts collecting data that they have and send it to driver program which then stores in the logs. 

# pyspark join
-> https://sparkbyexamples.com/pyspark/pyspark-join-explained-with-examples/

# pyspark sortby
-> https://sparkbyexamples.com/pyspark/pyspark-orderby-and-sort-explained/

# you need to specify the join type such as "inner", "left" etc.

## task vs stage vs job
-> https://www.hadoopinrealworld.com/what-are-applications-jobs-stages-and-tasks-in-spark/
-> Task: Task is the smallest execution unit in Spark. A task in spark executes a series of instructions. For eg. reading data, filtering and applying map() on data can be combined into a task. Tasks are executed inside an executor.
-> Stage: A stage comprises several tasks and every task in the stage executes the same set of instructions. When Spark encounters a function that requires a shuffle it creates a new stage. Transformation functions like reduceByKey(), Join() etc will trigger a shuffle and will result in a new stage. Spark will also create a stage when you are reading a dataset.
-> Job: A job comprises several stages. A job is created, whenever you execute an action function like write().

## Broadcast & Accumulator
-> broadcasted variable is cached on executors
-> Use spark.sparkContext.broadcast(<data>)
-> Access broadcast data using broadcast_var.value
-> use accumulators inside actions. You can use acumulator inside foreach as it causes side-effects. you cant rely on transformation functions to update the accumulator as they may get terminated and get restarted by pyspark & this will manupulate the accumulator again which will result in wrong info.
-> where to use accumulators: https://www.hadoopinrealworld.com/what-are-accumulators-in-spark-when-and-when-not-to-use-them/
-> https://stackoverflow.com/questions/29494452/when-are-accumulators-truly-reliable
-> https://sparkbyexamples.com/pyspark/pyspark-accumulator-with-example/

## User Defined Functions(UDF)
-> if you create a udf then you can call the function anywhere in the script.
-> you can create a udf using functions.udf(<function>, <return_type(optional)>). Default return type is StringType()
-> The return type of UDF is Column type
-> print(get_movie_name_udf(func.lit(196))) will result in Column<'get_movie_name(196)'>
-> I tried passing an integer as argument in udf but I got this error ==> TypeError: Invalid argument, not a string or column: 196 of type <class 'int'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.
-> UDF's param can be of 2 types i.e. Column or string
-> use functions.lit() to convert integer/data to Column type
-> print(type(func.lit(196))) will result in <class 'pyspark.sql.column.Column'>
-> You have to resgister your function as udf to use it with SQL quries in spark sql
-> Usage ==> spark.udf.register(<function>)
-> Example sql query code ==> spark.sql("select Seqno, convertUDF(Quote) from QUOTE_TABLE").show(false)

## withColumn
-> You can add a new column or modify an existing column using withColumn
-> Usage ==> df.withColumn("column_name", <Column type value>)
-> In  the second params you can pass an UDF or a Column type value 

## Partitions
-> spark-spartitioning ==> https://sparkbyexamples.com/spark/spark-partitioning-understanding/
-> https://www.projectpro.io/article/how-data-partitioning-in-spark-helps-achieve-more-parallelism/297
-> spark.sql.shuffle.partitions vs spark.default.parallelism ==> https://sparkbyexamples.com/spark/difference-between-spark-sql-shuffle-partitions-and-spark-default-parallelism/
-> spark.default.parallelism was introduced with RDD hence this property is only applicable to RDD. The default value for this configuration set to the number of all cores on all nodes in a cluster, on local, it is set to the number of cores on your system.
-> Whereas spark.sql.shuffle.partitions was introduced with DataFrame and it only works with DataFrame, the default value for this configuration set to 200.
-> For RDD, wider transformations like reduceByKey(), groupByKey(), join() triggers the data shuffling. Use spark.sql.shuffle.partitions for shuffle operations.
-> When you running on local in standalone mode, Spark partitions data into the number of CPU cores you have on your system or the value you specify at the time of creating SparkSession object
-> On the HDFS cluster, by default, Spark creates one Partition for each block of the file. Version 2 Hadoop the HDFS block size is 128 MB. if you have 640 MB file and running it on Hadoop version 2, creates 5 partitions with each consists on 128 MB blocks (5 blocks * 128 MB = 640 MB). If you repartition to 10 then it creates 2 partitions for each block.
-> val rdd= sc.textFile (“file.txt”, 5)

The above line of code will create an RDD named textFile with 5 partitions. Suppose that you have a cluster with four cores and assume that each partition needs to process for 5 minutes. In case of the above RDD with 5 partitions, 4 partition processes will run in parallel as there are four cores and the 5th partition process will process after 5 minutes when one of the 4 cores, is free. The entire processing will be completed in 10 minutes and during the 5th partition process, the resources (remaining 3 cores) will remain idle. The best way to decide on the number of partitions in an RDD is to make the number of partitions equal to the number of cores in the cluster so that all the partitions will process in parallel and the resources will be utilized in an optimal way.

## SPARK Streaming ##
-> Doc: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html
-> Write Stream: https://medium.com/expedia-group-tech/apache-spark-structured-streaming-output-sinks-3-of-6-ed3247545fbc
-> When specifying folder is readStream, spark watches for files in that folder only, does not go into folder inside the current folder. For example if folder structure is like this
----- > students
----------> A
---------------> file.txt
& it's watching in students folder then it can't find file.txt which is inside folder A. 

# Streaming queries
-> query = df.writeStream.format("console").start()   # get the query object
-> query.id()          # get the unique identifier of the running query that persists across restarts from checkpoint data
-> query.runId()       # get the unique id of this run of the query, which will be generated at every start/restart
-> query.name()        # get the name of the auto-generated or user-specified name
-> query.explain()   # print detailed explanations of the query
-> query.stop()      # stop the query
-> query.awaitTermination()   # block until query is terminated, with stop() or with error
-> query.exception()       # the exception if the query has been terminated with error
-> query.recentProgress  # a list of the most recent progress updates for this query
-> query.lastProgress    # the most recent progress update of this streaming query


# Output mode
-> there are 3 outpute modes i.e. append, complete & update
-> append: Use append as output mode outputMode("append") when you want to output only new rows to the output sink. Filesink only support Append mode.
append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark.
-> complete: Use complete as output mode outputMode("complete") when you want to aggregate the data and output the entire results to sink every time. 
-> update: It is similar to the complete with one exception; update output mode outputMode("update") just outputs the updated aggregated results every time to data sink when new data arrives

-> need to add watermark for file sink -> https://learn.microsoft.com/en-us/azure/databricks/kb/streaming/append-output-not-supported-no-watermark

## Ports
Spark Master UI: http://localhost:8080
Spark Slave UI: http://localhost:8081
Spark Driver UI: http://localhost:4040(accessible only after a driver is started)
Spark Master Server: http://namenode:7077(not exposed to host)

### ZEPPLIN ###

-> cd /opt
-> curl -O https://dlcdn.apache.org/zeppelin/zeppelin-0.10.1/zeppelin-0.10.1-bin-netinst.tgz --insecure
-> tar -xf zeppelin-0.10.1-bin-netinst.tgz
-> rm -rf zeppelin*.tgz
// ignore: -> mv zeppelin-0.10.1-bin-netinst zeppelin
-> mv zeppelin-0.10.1-bin-netinst/* zeppelin
-> rm -rf zeppelin-0.10.1-bin-netinst
// ignore: -> echo "export ZEPPELIN_HOME=/opt/zeppelin" >> ~/.bashrc
// ignore: -> source ~/.bashrc
// ignore: -> echo "export PATH=$PATH:$ZEPPELIN_HOME/bin" >> ~/.bashrc
// ignore: -> source ~/.bashrc
-> zeppelin-daemon.sh start

# stop Zeppline
-> zeppelin-daemon.sh stop

# once you run "%spark.pyspark" in a paragraph, it creates a spark session which starts a driver program in the container and you can access driver UI on port 4040. If you create another script and create another session it starts another driver but the driver app UI starts in the next port i.e. 4041. In the docker compose file 4040 is exposed to host so, if your driver UI is running in another port then you'll have to expose it yourself. By default Zeppline will run spark jobs in local mode & uses all threads. You can configure spark interpreter to use yarn/spark master.

# you need to add "%spark.pyspark" to each block/para in zeppelin. Don't try to run an spark action on dataframe which is in another block. If you do this, a spark job will be created but it never seems to end. So, always write code in a single block/para.

### CASSANDRA ###
-> There are 3 types of strategies which are as follows
1. SimpleStrategy
2. LocalStrategy
3. NetworkTopologyStrategy 
-> SimpleStrategy & NetworkTopologyStrategy can be used by users. LocalStrategy can only be used by cassandra itself.
-> SimpleStrategy can be used if you have only 1 cluster in 1 data center.
-> NetworkTopologyStrategy is used if you have data in more than 1 data center
->  Show all keyspaces: select * from system_schema.keyspaces;
-> Create a keyspace: create keyspace movielens with replication={'class': 'SimpleStrategy', 'replication_factor': 1};
-> Use a keyspace: use <keyspace>
-> create a table in keyspace: create table movie_average(movie_id int primary key, average_rating int, movie_name text);
-> see tables in a keyspace: select * from system_schema.tables where keyspace_name='movielens';
-> see column properties in a table: select * from system_schema.columns where keyspace_name='movielens' and table_name='movie_average';

### Air Flow ###
-> conda install apache-airflow=2.3.3 -y
-> conda install -c anaconda psycopg2=2.9.3 -y
-> cd ~/airflow
-> airflow db init
-> airflow users create \
    --username admin \
    --password admin \
    --firstname admin \
    --lastname admin \
    --role Admin \
    --email mrugankray@gmail.com

-> source ~/.bashrc
// ignore: -> echo "export AIRFLOW_HOME=~/airflow" >> ~/.bashrc

-> rm -rf airflow-scheduler.pid
-> rm -rf airflow-webserver-monitor.pid 
-> rm -rf airflow-webserver.pid

OR
-> rm -rf *.pid
# installing airflow spark provider
-> conda install -c conda-forge apache-airflow-providers-apache-spark=3.0.0 -y
# installing airflow ssh provider
-> conda install -c conda-forge apache-airflow-providers-ssh=3.2.0 -y
-> airflow webserver -p 3000 -D --workers 1
-> airflow scheduler -D

-> delete pids
-> kill -9 process_id

## install ssh
//ignore: -> login into hive-server
//ignore: -> apt-get update
//ignore: -> apt-get install openssh-server -y
//ignore: -> service ssh start

## reset root password
//ignore:-> passwd root
//ignore:-> set it to 1234

## change ssh conf
// ignore: -> nano /etc/ssh/sshd_config
// ignore: -> modify -> PermitRootLogin yes
// ignore: -> modify -> PasswordAuthentication yes

## login using ssh
-> ssh root@172.21.0.12

## Spark Submit Operator 
-> https://airflow.apache.org/docs/apache-airflow-providers-apache-spark/1.0.1/_api/airflow/providers/apache/spark/operators/spark_submit/index.html
## Config airflow to run spark
-> https://medium.com/swlh/using-airflow-to-schedule-spark-jobs-811becf3a960

# Once airflow spark provider is installed, a spark_default connection is available in admin/Connections. It run the spark job in YARN. Click on edit & set the queue to default in extras field.

# You can create another spark connection & add spark://namenode as host & 7077 in port. This will run spark job in stand alone mode.

## Sqoop commands to start a job ##
# command to import ratings data
# create a job
-> sqoop job --create ratings_increment_import -- import --connect jdbc:postgresql://external_postgres_db/external --username external --password external --table ratings --target-dir /user/admin/learning/spark/importing_ratings_from_postgres --check-column id --incremental append --last-value 0 --m 1

# execute a job
-> sqoop job --exec ratings_increment_import

# delete the job
-> sqoop job --delete ratings_increment_import

# list all jobs
-> sqoop job --list

# add rating
-> INSERT INTO ratings(user_id, movie_id, rating, epoch) VALUES (1000, 242, 1, 1669292788);

# again exec ratings_increment_import job
-> sqoop job --exec ratings_increment_import

-> sqoop creates a new file in importing_ratings_from_postgres hdfs directory & stores the new data in that file

### Flume ###
-> login to namenode
-> cd /opt
-> curl -O https://dlcdn.apache.org/flume/1.11.0/apache-flume-1.11.0-bin.tar.gz --insecure
-> tar -xf apache-flume-1.11.0-bin.tar.gz
-> rm -rf apache-flume-1.11.0-bin.tar.gz
// ignore: -> mv apache-flume-1.11.0-bin flume
-> mv apache-flume-1.11.0-bin/* flume
-> rm -rf apache-flume-1.11.0-bin
-> cd flume/lib
-> rm -rf guava-11.0.2.jar
-> mv log4j-slf4j-impl-2.18.0.jar ../
-> curl -O https://repo1.maven.org/maven2/com/google/guava/guava/31.1-jre/guava-31.1-jre.jar

## Flume Doc
-> https://flume.apache.org/FlumeUserGuide.html

## Run flume
-> flume-ng agent --conf conf --conf-file /opt/flume/conf/flume.conf --name a1 -Dflume.root.logger=INFO,console

## Run Flume as a service
-> service flume-agent start

### BUILD NAMENODE ###
-> sudo docker build -t namenode-spark-airflow-flume-zepplin:1.0 .

### BUILD HIVE SERVER ###
-> sudo docker build -t hive-server-sqoop:1.0 .